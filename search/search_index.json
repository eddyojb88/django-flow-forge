{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Django Flow Forge","text":"<p>The Data and ML Ops ecosystems are cluttered with overly complex solutions and vendor lock-ins that all carry significant pitfalls. Django Flow Forge aims to be a beacon of pragmatism when forging together your data ops and ML work flows.</p>"},{"location":"#designed-for-data-scientists-and-data-engineers","title":"Designed for Data Scientists and Data Engineers","text":"<p>By building upon the Django frameowrk, a mature web server framework with modules that already solve a lot of problems in ML Ops, this plugin module aims to streamline tech stacks within these fields, focusing on standardization and flexibility as a project grows.</p> <p>Django has a steeper learning curve than some other tools such as Luigi or Kedro, but as most data science projects grow, they typically end up requiring a production ready pipeline process and to serve the model in some way, leading you to despair as you had already invested in the tool that said would solve your problems.</p> <p>Using this tool helps minimise the long term technical risk of most ml ops and data science projects.</p> <p></p>"},{"location":"#features","title":"Features","text":"<ul> <li>Define your pipelines as a series of tasks that are database backed and can run in sync or async (using Celery)</li> <li>Visualize your pipelines for stakeholders, both in planning and development phases (inspired by Kedro)</li> <li>Task monitoring and visualization of outputs and failures</li> <li>Compare data science experiment results easily (inspired by Kedro)</li> <li>Serve machine learning models efficiently</li> <li>A familiar environment for Django users, opening the door to a large developer community</li> <li>Leverage Django's robust security and scalability features, including Django Celery and flexbility to use Kubernetes engines, without the worry of vendor lock-in</li> <li>Encourages standardisation of data science work flows</li> <li>Encourages teams to move away from Jupyter Notebooks, which cause a plethora of issues</li> </ul>"},{"location":"#motivation","title":"Motivation","text":"<p>It is necessary in data science projects to:</p> <ul> <li>Design and keep track of data science projects in a way that can be easily communicated to team members and stakeholders</li> <li>Offer scalability when gathering data or model training in order to find (much) better solutions to complex problems</li> </ul> <p>An ML system should also be simple enough that colleagues can:</p> <p>(1) Find out when it isn\u2019t working properly</p> <p>(2) Make small changes to it</p> <p>(3) Redeploy the model</p> <p>In most cases, you dont need any of the highly specialised ML Ops tools out there. Our analysis revealed that in many cases, issues in machine learning operations can be attributed to bad code design and the use of notebooks.</p>"},{"location":"#inspiration","title":"Inspiration","text":"<p>This package is inspired by the following articles that challenge the prevailing ML Ops narrative, drawing inspiration from critical discussions on the necessity and implementation of MLOps. This module seeks to avoid the pitfalls of overcomplication and excessive relaiance on overly specialized tools:</p> <ul> <li>No, you dont need ML Ops</li> <li>Do you need ML Ops</li> <li>I dont like Notebooks/Collab</li> </ul>"},{"location":"#the-state-of-the-ml-ops-ecosystem","title":"The State of the ML Ops ecosystem","text":"<p>The current MLOps landscape is cluttered with overengineered solutions. Analysis reveals a stark contrast in codebase size and functionality, with Django Flow Forge offering a lean yet powerful alternative to help solve end to end solutions (&lt;4k of code vs Kedro with 430k lines of code!). Yet all other tools analysed have significant deficiencies. The features of Django Flow Forge are designed to meet a vision for a more accessible and flexible ML operational environment.</p>"},{"location":"#why-not-use-eg-apache-beam","title":"Why not use e.g. Apache Beam?","text":"<p>Beam is great for realtime applications and massively scaling pipelines.  However, it isn't ideal for standardisation of data science workflows, monitoring and communicating or serving. That said, there is no reason you cannot use Apache Beam within this package.</p>"},{"location":"authorization/","title":"Authentication and Authorization","text":"<p>Every view in Flow Forge is protected in production settings by default.</p> <p>In order to access the views one of 3 conditions need to be met:</p> <ul> <li>DEBUG = True in the projects settings.py </li> <li>User is a superuser (this can be overridden to False if required using the decorator)</li> <li>User is part of a Group that has the <code>Django_Flow_Forge | flow | Can access admin</code> permission which can be assigned via the Django Admin or programatically (please search via the web if this process is unfamiliar).</li> </ul> <p>If you end up writing custom views with your Flows then you can wrap your own views with a handy decorator:</p> <p><code>from django_flow_forge.authorization import user_has_permission</code></p> <p>If someone requires a Class mixin this can be imported from:</p> <p><code>from django_flow_forge.authorization import FlowForgePermissionMixin</code></p>"},{"location":"considerations/","title":"Considerations","text":""},{"location":"considerations/#what-are-flows","title":"What are Flows?","text":"<p>Flows in Django Flow Forge are database backed sequences of tasks designed to help automate data operations and machine learning operational workflows. Each Flow consists of multiple Tasks that can be executed in a specified order. Tasks can contain logic that can be run in e.g. Celery or upscaled in Kuberenetes.  Flows are designed to simplify complex data processing, machine learning model training, evaluation, and deployment processes by encapsulating them into manageable, repeatable, and scalable operations.</p>"},{"location":"considerations/#how-are-flows-defined","title":"How are Flows defined?","text":"<p><code>register_task_pipeline(         flow_name='pipeline_simple_ml',          clear_existing_flow_in_db=True,         pipeline = {                     'fetch_data2': {'function': fetch_data2, 'depends_on': []},                     'clean_data': {'function': clean_data, 'depends_on': ['fetch_data1', 'fetch_data2']},                     'analyze_data': {'function': analyze_data, 'depends_on': ['clean_data']},                     'train_model': {'function': train_model, 'depends_on': ['analyze_data']},                    }     )</code> A flow is defined by registering a series of tasks, where each task is associated with a specific function to execute. Additional metadata describing its dependencies on other tasks is also defined in order to understand the task order.  The flow's tasks are stored and managed in the database, enabling dynamic modification and scalable execution and tracking.</p> <p>WARNING: It is important to set the 'depends_on' properly when parrelising tasks. If you don't set dependencies then the system will think they are all fine to run at the same time.</p> <p>Also, when writing your Tasks, a good habit is to make sure they all have functional independence i.e. don't return large objects, especially not Pandas DataFrames, which aren't JSON serializable so should only be used within the function. Store your files to your system or even better database.</p>"},{"location":"considerations/#nested-tasks","title":"Nested tasks","text":"<p>It's crucial to recognize that, as of the current implementation, nested tasks in Django-Flow-Forge are utilized primarily for visualization purposes. This means that while nested tasks significantly aid in depicting the structure and dependencies of a workflow in a more intuitive and detailed manner, they do not alter the execution logic of the pipeline. The primary execution flow treats these nested tasks as part of the linear sequence of tasks, irrespective of their hierarchical representation in the visualization.</p>"},{"location":"considerations/#visualising-your-flows-in-concept","title":"Visualising your flows in concept","text":"<p>When running the server, you can go here in order to view your flows in concept: <code>http://localhost:8005/django_flow_forge/conceptual-dag-viz/</code></p> <p>This is great for communicating to stakeholders what it is that you are working on.</p>"},{"location":"considerations/#scheduled-tasks","title":"Scheduled Tasks","text":"<p>It is recommended to consider Celery Beat with Django for this. There is lots of documentation on this and the beauty of this module is that your choice of solution is not necessarily prescribed.</p>"},{"location":"considerations/#parallelisation","title":"Parallelisation","text":"<p>This is always a tricky topic and so care is required in designing from the outset.</p> <p>This module was developed so that it can be flexible. Each Task in a Flow can be executed in parralel with others, so long as its dependencies have been met. </p> <p>To run a Task in Celery, this requires setting e.g. a @shared_task decorator to the specific functions that you want run in async and setting use_celery=True in the flow args (see example_project).</p> <p>You can also run jobs in parallel within a Task (see pipeline_simple_celery example in example_project)</p>"},{"location":"considerations/#parallelisation-within-parallelisation","title":"Parallelisation within Parallelisation","text":"<p>What is very tricky is running distributed tasks within distributed tasks. It can be done but not advised. If you really want to go down this route, you can use the <code>allow_join_result</code> option in Celery to achieve this.</p>"},{"location":"data_science_experiments/","title":"Data Science Experiments","text":""},{"location":"data_science_experiments/#storing-results-from-data-science-experiments","title":"Storing Results from Data Science Experiments","text":"<p>You can view examples for this in the <code>ml_grid_search.py</code> script.</p> <p>Results are stored to JSON fields within the database using the Task object.</p> <p>These results are automatically viewable in the dashboard.</p>"},{"location":"data_science_experiments/#visualizations-of-data-science-experiments","title":"Visualizations of Data Science Experiments","text":"<p>Only the table of results is displayed in the Pipeline Monitor page <code>localhost:8000/django_flow_forge/task-runs-viz</code> but you can easily create your own visualisation page in Django by reading from the relevant Task objects for given Flows of interest.</p> <p>Data science experiments can easily be plotted by extracting results from the MLResults model in django_flow_forge  <code>from django_flow_forge.models import MLResult</code>and you can send the data to the view template and render using something like Plotly or echarts. This is easily achieved using a Copilot.</p>"},{"location":"debugging/","title":"Debugging","text":""},{"location":"debugging/#running-selective-tasks-in-a-pipeline","title":"Running selective tasks in a Pipeline","text":"<p>If you only want to run a small portion of the pipeline, you can comment out registered tasks of the flow. When you go to run the flow, you can insert the <code>ignore_task_dependencies</code> keyword:</p> <p><code>run_flow('your_flow', ignore_task_deps_in_debug_mode=True, **kwargs)</code></p> <p>Warning: Be aware that this currently breaks the DAG in the visualisation tools as it doesn't yet handle missing dependencies.</p>"},{"location":"flows_dashboard/","title":"Flow Dashboard","text":""},{"location":"flows_dashboard/#features","title":"Features","text":"<ul> <li>Task monitoring (there is auto-checkpoints between Tasks)</li> <li> <p>Visualize a Flow in terms of its Tasks, dependecy tree and Task status. Clicking on nodes allows you to inspect the status and output</p> </li> <li> <p>Outputs for your machine learning algorithms</p> </li> </ul> <p>You are required to define what the machine learning outputs are, which are saved to the MLResults object.</p> <p>At the moment, this aspect is hands off, recognising that the world of ML is rapidly changing and that different domains require different evaluation metrics recorded. This is why this aspect intends to be replaceable and extensible. With Django-HTMX, chart.js and a bit of Chatgpt, it is very simple to code a visualisation.</p>"},{"location":"installation/","title":"Installation","text":"<p>If you are not new to this project, you can install via <code>pip install django-flow-forge</code></p>"},{"location":"installation/#quickstart-example-project","title":"Quickstart Example Project","text":"<p>It is recommended to clone the Github repo in order to view the <code>example_project</code> in action.</p>"},{"location":"installation/#docker","title":"Docker","text":"<p>Once cloned, the quickest route to viewing if you have Docker installed is:</p> <p><code>docker compose -f docker-compose-local.yml up</code></p> <p>Then connect in to the django container</p>"},{"location":"installation/#pip","title":"Pip:","text":"<p>Once the repo has been cloned, you can either install the requirements in your virtual env (virtualenv, conda etc). e.g.</p> <p><code>pip install -r requirements.txt</code></p> <p>However, if you are interested in viewing how tasks in async are run in the <code>pipeline_simple_with_celery.py</code> example, you will also need a task server running, such as RabbitMQ or Redis. The recommended way to view this simply is to use the docker-compose file in the step above.</p>"},{"location":"installation/#applying-migration-to-database","title":"Applying Migration to Database","text":"<p>The database is a sqlite file with code in the example project to modify it so that it can allow celery to interact with it. N.B. a production solution is a database like Postgres or warehouse like Snowflake.</p> <p>Go in to the <code>example_project</code> directory and migrate the DB:</p> <p><code>python manage.py migrate</code></p> <p>Also make sure to create a superuser to view the django flow forge pages since they are protected behind authentication:</p> <p><code>python manage.py createsuperuser</code></p>"},{"location":"installation/#enable-registering-of-flows","title":"Enable registering of flows","text":"<p>Each pipeline has to be imported in some way. These are currently in <code>example_project.example_app.pipelines</code></p>"},{"location":"installation/#run-the-server-and-start-tasks","title":"Run the server and start tasks","text":"<p><code>python manage.py runserver 0.0.0.0:8000</code></p> <p>Now go to:</p> <p><code>http://localhost:8005/example/</code></p> <p>You will see a list of tasks. You can click on each of them one at a time. Unfortunately, the page isn't async so when you run it, you will need to wait before receiving a message saying that the 'Task executed successfully'.</p> <p>Continue clicking each example. The ML grid search example will take a minute</p>"},{"location":"installation/#if-using-celery-optional","title":"If using Celery (Optional):","text":"<p>If you want to see the examples in async, in a new terminal window:</p> <p><code>cd example_project</code></p> <p><code>celery -A example_app  worker --loglevel=info</code></p>"},{"location":"installation/#view-the-results-in-a-dashboard","title":"View the results in a dashboard:","text":"<p>You will need to be logged in first. Login via <code>http://localhost:8005/admin</code></p> <p>Now go to:</p> <p><code>http://localhost:8005/django_flow_forge/task-runs-viz/</code></p> <p>If wanting to conceptualize a task for stakeholders before or during development, you can view the pipeline in concept by going to:</p> <p><code>http://localhost:8005/django-flow-forge/conceptual-dag-viz/</code></p> <p></p> <p>More details on this can be found in Dashboard section</p>"},{"location":"installation/#running-tasks-with-no-web-server","title":"Running tasks with no web server","text":"<p>A great feature of this module is that you can develop and run tasks without the web server. See the  <code>run_tasks_no_server.py</code> script to see how this works.</p>"},{"location":"strategies/","title":"System Design Strategies","text":"<p>Given that there are infinite variations, this page documents some example strategies when using this module in case you are wondering whether it is an appropriate choice for your project.</p>"},{"location":"strategies/#long-running-flows","title":"Long Running Flows","text":""},{"location":"strategies/#long-running-high-compute-intensity","title":"Long running, high compute intensity:","text":"<p>If you are, for example, running a task once a day that is intensive for a period of time, you could run another replica of your django project and develop a small API that accepts a call to run a Pipeline. The API could be built in Django-Ninja.</p> <p>You could also have a number of replicas behind a load balancer (e.g. AWS ECS or Kubernetes)</p>"},{"location":"strategies/#long-running-low-compute-intensity","title":"Long running, low compute intensity:","text":"<p>If it is a long running task but not that compute intensive, you could call the Pipeline in async, or if calling with a view. You could use django-channels to offload the task out of the main loop and provide updates to a user in an app.</p>"},{"location":"strategies/#realtime-streaming-data","title":"Realtime Streaming Data","text":"<p>Consider Apache-Beam, either within a Task in the django-flow-forge framework or completely seperately.</p>"}]}